{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "import tensorboard\n",
    "from tensorflow.keras import backend as K\n",
    "from Sum_Tree import SumTree\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#tf.config.optimizer.set_jit(True)\n",
    "#tf.config.experimental.enable_mlir_bridge()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_tensorboard():\n",
    "    import os\n",
    "    tensor_board_sess_path = 'C:/Users/Eaj59/AppData/Local/Temp/.tensorboard-info/'\n",
    "    temp_sess_files = os.listdir(tensor_board_sess_path)\n",
    "    for i in temp_sess_files:\n",
    "        temp_file_path = os.path.join(tensor_board_sess_path,i)\n",
    "        os.remove(temp_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clean_tensorboard()\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir 'C:/Users/Eaj59/Documents/RL_Projects/Project_2_DRL/log_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self,env_name,model_name,enable_PER=True,enable_DDQN=True,enable_tb_logging=True,enable_dueling=True):\n",
    "        self.env = gym.make(env_name)\n",
    "        #self.env.seed(0)\n",
    "        self.env_state = self.env.reset()\n",
    "        self.lr = 0.1\n",
    "        self.gamma = .99\n",
    "        self.epsilon_decay_rate = .8\n",
    "        self.initializer = tf.keras.initializers.HeUniform()\n",
    "        self.loss_parameter = 1.0\n",
    "        self.loss_fn = tf.keras.losses.Huber(delta=self.loss_parameter)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.n_states = self.env.observation_space.shape[0]\n",
    "        self.tau = 0.10\n",
    "        self.batch_size = 32\n",
    "        self.epsilon = .5\n",
    "        self.min_epsilon = .05\n",
    "        self.dueling_method = 'average'\n",
    "        self.enable_tb_logging = enable_tb_logging\n",
    "        self.enable_dueling = enable_dueling\n",
    "        self.episodic_rewards = 0\n",
    "        self.episode_counter = 0\n",
    "        self.episode_time_step_counter = 0\n",
    "        self.enable_DDQN = True\n",
    "        self.enable_PER = enable_PER\n",
    "        self.model_name = model_name\n",
    "        self.target_update_method = 'hard'\n",
    "        self.online = self.make_nn()\n",
    "        self.target = self.make_target()\n",
    "        self.capacity = 5000\n",
    "        \n",
    "        \n",
    "        if self.enable_PER:\n",
    "            self.buffer = Prioritized_Buffer(self.capacity)\n",
    "        else:\n",
    "            self.buffer = ReplayBuffer(self.capacity)\n",
    "            \n",
    "        if self.enable_tb_logging:\n",
    "            self.online_run_id, self.online_tensor_board_callback = self.create_tensor_board_callback(model_name=self.model_name)\n",
    "            self.training_steps_counter = 0\n",
    "        \n",
    "    def reset_env(self):\n",
    "        self.episode_counter+=1\n",
    "        if self.enable_tb_logging:\n",
    "            with self.online_tensor_board_callback.as_default():\n",
    "                tf.summary.scalar(name='Episode_Reward',data=self.episodic_rewards,step=self.episode_counter)\n",
    "                tf.summary.scalar(name='Episode_Time_Steps',data=self.episode_time_step_counter,step=self.episode_counter)\n",
    "        self.episodic_rewards = 0\n",
    "        self.episode_time_step_counter = 0\n",
    "        self.env_state = self.env.reset()\n",
    "        \n",
    "    \n",
    "    def __dict__(self):\n",
    "        parameter_dict = {'Model_Name': self.model_name,\n",
    "                        'Date': time.strftime('%Y_%m_%D'),\n",
    "                        'DDQN': self.enable_DDQN,\n",
    "                        'Dueling': self.enable_dueling,\n",
    "                        'Gamma': self.gamma,\n",
    "                        'Inital_Epsilon':self.epsilon,\n",
    "                        'Epsilon_Decay_Rate': self.epsilon_decay_rate,\n",
    "                        'Epsilon_Minimum': self.min_epsilon,\n",
    "                        'Optimizer': self.optimizer.__dict__['_name'],\n",
    "                        'Learning_Rate': self.lr,\n",
    "                        'Loss_Function' : self.loss_fn.__dict__['name'],\n",
    "                        'Loss_Parameter': self.loss_parameter}\n",
    "        return str(parameter_dict)\n",
    "        \n",
    "\n",
    "    def create_tensor_board_callback(self,model_name):\n",
    "        run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "        run_id = model_name+'_'+run_id\n",
    "        base_dir = 'C:/Users/Eaj59/Documents/RL_Projects/Project_2_DRL'\n",
    "        os.chdir('C:/Users/Eaj59/Documents/RL_Projects/Project_2_DRL/log_dir')\n",
    "        os.mkdir(run_id)\n",
    "        os.chdir(run_id)\n",
    "        text_file_name = 'model_summary_' + model_name +'.txt'\n",
    "        text_file_name2 = 'model_hyper_parameters_' + model_name +'.txt'\n",
    "        f = open(text_file_name,\"w+\")\n",
    "        f.write(self.online.to_json())\n",
    "        f.close()\n",
    "        f2 = open(text_file_name2,\"w+\")\n",
    "        f2.write(self.__dict__())\n",
    "        f2.close()\n",
    "        os.chdir(base_dir)\n",
    "        root_log_dir = os.path.join(os.curdir,'log_dir')\n",
    "        model_cb_path = os.path.join(root_log_dir,run_id)\n",
    "        file_writer = tf.summary.create_file_writer(model_cb_path)\n",
    "        \n",
    "        return run_id, file_writer\n",
    "    \n",
    "    def make_nn(self):\n",
    "        \n",
    "        if self.enable_dueling:\n",
    "            Dueling_Input = tf.keras.layers.Input(shape=(self.n_states,),name='Dueling_Input')\n",
    "            #layer_1_Dense = tf.keras.layers.Dense(128,activation='swish',kernel_initializer=self.initializer,name='layer_1_Dense')(Dueling_Input)\n",
    "            layer_2_Dense = tf.keras.layers.Dense(64,activation='swish',kernel_initializer=self.initializer,name='layer_2_Dense')(Dueling_Input)\n",
    "            layer_3_Dense = tf.keras.layers.Dense(32,activation='swish',kernel_initializer=self.initializer,name='layer_3_Dense')(layer_2_Dense)\n",
    "            #Advantage_Layer = tf.keras.layers.Dense(32,activation='swish',kernel_initializer=self.initializer,name='Advantage_Layer')(layer_3_Dense)\n",
    "            #Value_Layer = tf.keras.layers.Dense(32,activation='swish',kernel_initializer=self.initializer,name='Value_Layer')(layer_3_Dense)\n",
    "            Advantage_Layer2 = tf.keras.layers.Dense(self.n_actions,activation='swish',kernel_initializer=self.initializer,name='Advantage_Layer2')(layer_3_Dense)\n",
    "            Value_Layer2 = tf.keras.layers.Dense(1,activation='swish',kernel_initializer=self.initializer,name='Value_Layer2')(layer_3_Dense)\n",
    "\n",
    "            if self.dueling_method == 'average':\n",
    "                Value_Expanded = tf.keras.layers.Lambda(lambda s: K.expand_dims(s[:,0],-1),output_shape=(self.n_actions,))(Value_Layer2)\n",
    "                Average_Advantage = tf.keras.layers.Lambda(lambda a: a[:,:] - K.mean(a[:,:],keepdims=True),output_shape=(self.n_states,))(Advantage_Layer2)\n",
    "                online_output = tf.keras.layers.Add()([Value_Expanded,Average_Advantage])\n",
    "\n",
    "\n",
    "            elif self.dueling_method == 'sum':\n",
    "                Value_Expanded = tf.keras.layers.Lambda(lambda s: K.expand_dims(s[:,0],-1),output_shape=(self.n_actions,))(Value_Layer2)\n",
    "                Average_Advantage = tf.keras.layers.Lambda(lambda a: a[:,:],output_shape=(self.n_states,))(Advantage_Layer2)\n",
    "                online_output = tf.keras.layers.Add()([Value_Expanded,Average_Advantage])\n",
    "\n",
    "            else:\n",
    "                Value_Expanded = tf.keras.layers.Lambda(lambda s: K.expand_dims(s[:,0],-1),output_shape=(self.n_actions,))(Value_Layer2)\n",
    "                Average_Advantage = tf.keras.layers.Lambda(lambda a: a[:,:] - K.max(a[:,:],keepdims=True),output_shape=(self.n_states,))(Advantage_Layer2)\n",
    "                online_output = tf.keras.layers.Add()([Value_Expanded,Average_Advantage])\n",
    "\n",
    "            online = tf.keras.Model(Dueling_Input,online_output)\n",
    "            online.compile(loss=self.loss_fn,optimizer=self.optimizer,metrics=['accuracy'])\n",
    "            online.summary()\n",
    "            return online\n",
    "\n",
    "        else:\n",
    "            online = tf.keras.models.Sequential()\n",
    "            online.add(tf.keras.layers.Dense(128,input_dim=self.n_states,activation='swish',kernel_initializer=self.initializer))\n",
    "            online.add(tf.keras.layers.Dense(64, activation='swish',kernel_initializer=self.initializer))\n",
    "            online.add(tf.keras.layers.Dense(32, activation='swish',kernel_initializer=self.initializer))\n",
    "            online.add(tf.keras.layers.Dense(self.n_actions,activation='linear',kernel_initializer=self.initializer))\n",
    "            online.compile(loss=self.loss_fn,optimizer=self.optimizer,metrics=['accuracy'])\n",
    "            return online\n",
    "        \n",
    "        \n",
    "    \n",
    "    def make_target(self):\n",
    "        target = tf.keras.models.clone_model(self.online)\n",
    "        target.set_weights(self.online.get_weights())\n",
    "        return target\n",
    "        \n",
    "        \n",
    "    def epsilon_greedy_policy(self):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return np.random.randint(low=0,high=self.n_actions)\n",
    "        else:\n",
    "            Q_values = self.online.predict(np.reshape(self.env_state,(1,len(self.env_state))))\n",
    "            return np.argmax(Q_values[0])\n",
    "        \n",
    "    def apply_epsilon_decay(self):\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay_rate,self.min_epsilon)   \n",
    "        \n",
    "\n",
    "    def play_one_step(self):\n",
    "        state = np.array(self.env_state)\n",
    "        action = self.epsilon_greedy_policy()\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        \n",
    "        if self.enable_PER:\n",
    "            self.buffer.add_exp_per(state=state, action=action, reward=reward, next_state=next_state, done=done)\n",
    "        else:\n",
    "            self.buffer.add_exp(state=state, action=action, reward=reward, next_state=next_state, done=done)\n",
    "        \n",
    "        self.env_state = next_state\n",
    "        self.episodic_rewards += reward\n",
    "        self.episode_time_step_counter += 1\n",
    "        return state, action, reward, next_state, done\n",
    "        \n",
    "         \n",
    "    def training_step(self):\n",
    "        \n",
    "        if self.buffer.__len__()<self.batch_size:\n",
    "            batch_size_instance = self.buffer.__len__()\n",
    "        else:\n",
    "            batch_size_instance = self.batch_size\n",
    "        \n",
    "        if self.enable_PER:\n",
    "            tree_index, weights,states,actions,rewards,next_states,dones = self.buffer.sample_PER(batch_size=batch_size_instance)\n",
    "        else:\n",
    "            states,actions,rewards,next_states,dones = self.buffer.exp_sample(batch_size=batch_size_instance)\n",
    "            weights = None\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.enable_DDQN:\n",
    "            online_state_prediction = self.online.predict(states)\n",
    "            online_state_prediction_array = np.array(online_state_prediction)\n",
    "            online_next_state_prediciton = self.online.predict(next_states)\n",
    "            target_next_state_prediction = self.target.predict(next_states)\n",
    "            online_next_state_arg_max = tf.argmax(online_next_state_prediciton,axis=1)\n",
    "            double_learning_mask = tf.one_hot(online_next_state_arg_max,depth=self.n_actions)\n",
    "            masked_target_qs = tf.reduce_sum(target_next_state_prediction*double_learning_mask,axis=1,keepdims=True)\n",
    "            q_update = rewards + (1 - dones) * self.gamma * masked_target_qs\n",
    "            q_update = tf.squeeze(q_update,axis=-1)\n",
    "        \n",
    "        else:\n",
    "            #states,actions,rewards,next_states,dones = self.buffer.exp_sample(batch_size=batch_size_instance)\n",
    "            online_state_prediction = self.online.predict(states)\n",
    "            online_state_prediction_array = np.array(online_state_prediction)\n",
    "            target_max_next_state_action_value_prediction = np.amax(self.target.predict(next_states),axis=1,keepdims=True)\n",
    "            q_update = rewards + (1 - dones) * self.gamma * target_max_next_state_action_value_prediction\n",
    "            q_update = tf.squeeze(q_update,axis=-1)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        if self.enable_PER:\n",
    "            absolute_td_errors = []\n",
    "            for i in range(0,online_state_prediction.shape[0]):\n",
    "                absolute_td_errors.append(abs(online_state_prediction_array[i,actions[i]]- q_update[i]))\n",
    "                online_state_prediction_array[i,actions[i]] = q_update[i]\n",
    "            self.buffer.update_td_batch(tree_index,absolute_td_errors)\n",
    "        \n",
    "        else:\n",
    "            for i in range(0,online_state_prediction.shape[0]):\n",
    "                online_state_prediction_array[i,actions[i]] = q_update[i]\n",
    "        \n",
    "    \n",
    "        if self.enable_tb_logging:\n",
    "            metrics_output = self.online.train_on_batch(states,online_state_prediction_array,sample_weight=weights,reset_metrics=True)\n",
    "            self.training_steps_counter+=1\n",
    "            with self.online_tensor_board_callback.as_default():\n",
    "                tf.summary.scalar(name='Batch_Huber_Loss',data=metrics_output[0],step=self.training_steps_counter)\n",
    "                tf.summary.scalar(name='Accuracy',data=metrics_output[1],step=self.training_steps_counter)\n",
    "                tf.summary.scalar(name='Epsilon',data=self.epsilon,step=self.training_steps_counter)\n",
    "        else:\n",
    "            self.online.train_on_batch(states,online_state_prediction_array,sample_weight=weights)\n",
    "            \n",
    "     \n",
    "    def update_target_network(self):\n",
    "        \n",
    "        if self.target_update_method == 'soft':\n",
    "            target_weights_current = self.target.get_weights()\n",
    "            online_weights_current = self.online.get_weights()\n",
    "            layer_counter = 0\n",
    "            for tw, ow in zip(target_weights_current,online_weights_current):\n",
    "                new_target_weights = (1-self.tau)*tw + (self.tau)*ow\n",
    "                target_weights_current[layer_counter] = new_target_weights\n",
    "                layer_counter += 1\n",
    "            self.target.set_weights(target_weights_current)\n",
    "        \n",
    "        else:\n",
    "            self.target.set_weights(self.online.get_weights())\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self,max_len):\n",
    "        self.max_len = max_len\n",
    "        self.buffer = deque(maxlen=self.max_len)\n",
    "        \n",
    "    \n",
    "    def add_exp(self,state,action,reward,next_state,done):\n",
    "        exp = (state,action,reward, next_state, done)\n",
    "        if len(self.buffer)<= self.max_len:\n",
    "            self.buffer.append(exp)\n",
    "        else:\n",
    "            self.buffer[0] = exp\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def exp_sample(self,batch_size):\n",
    "        indices = np.random.randint(len(self.buffer), size=batch_size)\n",
    "        batch = [self.buffer[index] for index in indices]\n",
    "        states, actions, rewards, next_states, dones = [np.array([experience[entry] for experience in batch])for entry in range(5)]\n",
    "        return states, actions, rewards[:,np.newaxis], next_states, dones[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Prioritized_Buffer():\n",
    "    \n",
    "    def __init__(self,capacity):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = 0.6\n",
    "        self.beta = 0.1\n",
    "        self.td_constant = 0.01\n",
    "        self.beta_rate = 0.99992\n",
    "        self.tree = SumTree(self.capacity)\n",
    "        self.upper_bound_error = 1.0\n",
    "        \n",
    "        \n",
    "    def add_exp_per(self,state,action,reward,next_state,done):\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.upper_bound_error\n",
    "        experience = (state,action,reward, next_state, done)\n",
    "        self.tree.add(max_priority,experience)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return np.count_nonzero(self.tree.data_store)\n",
    "    \n",
    "    def update_beta(self):\n",
    "        self.beta = min(1.0,self.beta*self.beta_rate**-1)\n",
    "    \n",
    "    def sample_PER(self,batch_size):\n",
    "        tree_index_list = []\n",
    "        data_index_list = []\n",
    "        priority_list = []\n",
    "        priority_segment = self.tree.total_priority / batch_size\n",
    "        for i in range(batch_size):\n",
    "            start_uniform,end_uniform = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(start_uniform,end_uniform)\n",
    "            leaf_index, priority, data_index = self.tree.get_node(value)\n",
    "            tree_index_list.append(leaf_index)\n",
    "            data_index_list.append(data_index)\n",
    "            priority_list.append(priority)\n",
    "        priority_vector = np.array(priority_list)\n",
    "        probabilities = (priority_vector**self.alpha) / np.sum(priority_vector**self.alpha)\n",
    "        weights = (probabilities * batch_size)**-self.beta\n",
    "        weights = weights / np.max(weights)\n",
    "        batch = [self.tree.data_store[index] for index in data_index_list]\n",
    "        states, actions, rewards, next_states, dones = [np.array([experience[entry] for experience in batch])for entry in range(5)]\n",
    "        self.update_beta()\n",
    "        return tree_index_list, weights, states, actions, rewards[:,np.newaxis], next_states, dones[:,np.newaxis]\n",
    "    \n",
    "    def update_td_batch(self,tree_index,absolute_td_errors):\n",
    "        \n",
    "        absolute_td_errors = np.array(absolute_td_errors) + self.td_constant\n",
    "        for a, b in zip(tree_index,absolute_td_errors):\n",
    "            self.tree.update_tree(a,b)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Dueling_Input (InputLayer)      [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_2_Dense (Dense)           (None, 64)           320         Dueling_Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_3_Dense (Dense)           (None, 32)           2080        layer_2_Dense[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Value_Layer2 (Dense)            (None, 1)            33          layer_3_Dense[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Advantage_Layer2 (Dense)        (None, 2)            66          layer_3_Dense[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           Value_Layer2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2)            0           Advantage_Layer2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2)            0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,499\n",
      "Trainable params: 2,499\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_agent = Agent('CartPole-v0',model_name='PER_cart_pole_dqn',enable_PER=False,enable_tb_logging=False,enable_DDQN=True,enable_dueling=True)\n",
    "rewards_buffer = []\n",
    "max_episodes = 200\n",
    "mean_score_buffer = deque(maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode: 0, Episode_Reward: 11, 10_Episode_Reward_Avg 11\n",
      " Episode: 1, Episode_Reward: 11, 10_Episode_Reward_Avg 11\n",
      " Episode: 2, Episode_Reward: 12, 10_Episode_Reward_Avg 11\n",
      " Episode: 3, Episode_Reward: 38, 10_Episode_Reward_Avg 18\n",
      " Episode: 4, Episode_Reward: 22, 10_Episode_Reward_Avg 19\n",
      " Episode: 5, Episode_Reward: 10, 10_Episode_Reward_Avg 19\n",
      " Episode: 6, Episode_Reward: 19, 10_Episode_Reward_Avg 20\n",
      " Episode: 7, Episode_Reward: 10, 10_Episode_Reward_Avg 20\n",
      " Episode: 8, Episode_Reward: 9, 10_Episode_Reward_Avg 14\n",
      " Episode: 9, Episode_Reward: 10, 10_Episode_Reward_Avg 12\n",
      " Episode: 10, Episode_Reward: 8, 10_Episode_Reward_Avg 11\n",
      " Episode: 11, Episode_Reward: 8, 10_Episode_Reward_Avg 9\n",
      " Episode: 12, Episode_Reward: 10, 10_Episode_Reward_Avg 9\n",
      " Episode: 13, Episode_Reward: 11, 10_Episode_Reward_Avg 9\n",
      " Episode: 14, Episode_Reward: 11, 10_Episode_Reward_Avg 10\n",
      " Episode: 15, Episode_Reward: 11, 10_Episode_Reward_Avg 10\n",
      " Episode: 16, Episode_Reward: 12, 10_Episode_Reward_Avg 11\n",
      " Episode: 17, Episode_Reward: 14, 10_Episode_Reward_Avg 12\n",
      " Episode: 18, Episode_Reward: 13, 10_Episode_Reward_Avg 12\n",
      " Episode: 19, Episode_Reward: 9, 10_Episode_Reward_Avg 12\n",
      " Episode: 20, Episode_Reward: 10, 10_Episode_Reward_Avg 12\n",
      " Episode: 21, Episode_Reward: 38, 10_Episode_Reward_Avg 17\n",
      " Episode: 22, Episode_Reward: 30, 10_Episode_Reward_Avg 20\n",
      " Episode: 23, Episode_Reward: 9, 10_Episode_Reward_Avg 19\n",
      " Episode: 24, Episode_Reward: 10, 10_Episode_Reward_Avg 19\n",
      " Episode: 25, Episode_Reward: 10, 10_Episode_Reward_Avg 19\n",
      " Episode: 26, Episode_Reward: 12, 10_Episode_Reward_Avg 14\n",
      " Episode: 27, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 28, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 29, Episode_Reward: 11, 10_Episode_Reward_Avg 10\n",
      " Episode: 30, Episode_Reward: 12, 10_Episode_Reward_Avg 11\n",
      " Episode: 31, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 32, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 33, Episode_Reward: 11, 10_Episode_Reward_Avg 11\n",
      " Episode: 34, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 35, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 36, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 37, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 38, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 39, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 40, Episode_Reward: 19, 10_Episode_Reward_Avg 11\n",
      " Episode: 41, Episode_Reward: 9, 10_Episode_Reward_Avg 11\n",
      " Episode: 42, Episode_Reward: 9, 10_Episode_Reward_Avg 11\n",
      " Episode: 43, Episode_Reward: 9, 10_Episode_Reward_Avg 11\n",
      " Episode: 44, Episode_Reward: 9, 10_Episode_Reward_Avg 11\n",
      " Episode: 45, Episode_Reward: 10, 10_Episode_Reward_Avg 9\n",
      " Episode: 46, Episode_Reward: 10, 10_Episode_Reward_Avg 9\n",
      " Episode: 47, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 48, Episode_Reward: 8, 10_Episode_Reward_Avg 9\n",
      " Episode: 49, Episode_Reward: 8, 10_Episode_Reward_Avg 9\n",
      " Episode: 50, Episode_Reward: 8, 10_Episode_Reward_Avg 9\n",
      " Episode: 51, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 52, Episode_Reward: 9, 10_Episode_Reward_Avg 8\n",
      " Episode: 53, Episode_Reward: 11, 10_Episode_Reward_Avg 9\n",
      " Episode: 54, Episode_Reward: 8, 10_Episode_Reward_Avg 9\n",
      " Episode: 55, Episode_Reward: 11, 10_Episode_Reward_Avg 10\n",
      " Episode: 56, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 57, Episode_Reward: 84, 10_Episode_Reward_Avg 25\n",
      " Episode: 58, Episode_Reward: 10, 10_Episode_Reward_Avg 24\n",
      " Episode: 59, Episode_Reward: 10, 10_Episode_Reward_Avg 25\n",
      " Episode: 60, Episode_Reward: 10, 10_Episode_Reward_Avg 25\n",
      " Episode: 61, Episode_Reward: 67, 10_Episode_Reward_Avg 36\n",
      " Episode: 62, Episode_Reward: 11, 10_Episode_Reward_Avg 22\n",
      " Episode: 63, Episode_Reward: 41, 10_Episode_Reward_Avg 28\n",
      " Episode: 64, Episode_Reward: 11, 10_Episode_Reward_Avg 28\n",
      " Episode: 65, Episode_Reward: 9, 10_Episode_Reward_Avg 28\n",
      " Episode: 66, Episode_Reward: 8, 10_Episode_Reward_Avg 16\n",
      " Episode: 67, Episode_Reward: 11, 10_Episode_Reward_Avg 16\n",
      " Episode: 68, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 69, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 70, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 71, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 72, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 73, Episode_Reward: 8, 10_Episode_Reward_Avg 9\n",
      " Episode: 74, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 75, Episode_Reward: 10, 10_Episode_Reward_Avg 9\n",
      " Episode: 76, Episode_Reward: 11, 10_Episode_Reward_Avg 9\n",
      " Episode: 77, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 78, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 79, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 80, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 81, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 82, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 83, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 84, Episode_Reward: 8, 10_Episode_Reward_Avg 9\n",
      " Episode: 85, Episode_Reward: 10, 10_Episode_Reward_Avg 9\n",
      " Episode: 86, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 87, Episode_Reward: 10, 10_Episode_Reward_Avg 9\n",
      " Episode: 88, Episode_Reward: 10, 10_Episode_Reward_Avg 9\n",
      " Episode: 89, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 90, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 91, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 92, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 93, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 94, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 95, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 96, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 97, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 98, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 99, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 100, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 101, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 102, Episode_Reward: 8, 10_Episode_Reward_Avg 9\n",
      " Episode: 103, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 104, Episode_Reward: 8, 10_Episode_Reward_Avg 9\n",
      " Episode: 105, Episode_Reward: 8, 10_Episode_Reward_Avg 8\n",
      " Episode: 106, Episode_Reward: 9, 10_Episode_Reward_Avg 8\n",
      " Episode: 107, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 108, Episode_Reward: 10, 10_Episode_Reward_Avg 9\n",
      " Episode: 109, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 110, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 111, Episode_Reward: 10, 10_Episode_Reward_Avg 9\n",
      " Episode: 112, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 113, Episode_Reward: 15, 10_Episode_Reward_Avg 10\n",
      " Episode: 114, Episode_Reward: 11, 10_Episode_Reward_Avg 11\n",
      " Episode: 115, Episode_Reward: 9, 10_Episode_Reward_Avg 11\n",
      " Episode: 116, Episode_Reward: 10, 10_Episode_Reward_Avg 11\n",
      " Episode: 117, Episode_Reward: 9, 10_Episode_Reward_Avg 11\n",
      " Episode: 118, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 119, Episode_Reward: 10, 10_Episode_Reward_Avg 9\n",
      " Episode: 120, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 121, Episode_Reward: 11, 10_Episode_Reward_Avg 10\n",
      " Episode: 122, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 123, Episode_Reward: 12, 10_Episode_Reward_Avg 10\n",
      " Episode: 124, Episode_Reward: 11, 10_Episode_Reward_Avg 11\n",
      " Episode: 125, Episode_Reward: 10, 10_Episode_Reward_Avg 11\n",
      " Episode: 126, Episode_Reward: 10, 10_Episode_Reward_Avg 11\n",
      " Episode: 127, Episode_Reward: 11, 10_Episode_Reward_Avg 11\n",
      " Episode: 128, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 129, Episode_Reward: 8, 10_Episode_Reward_Avg 10\n",
      " Episode: 130, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 131, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 132, Episode_Reward: 10, 10_Episode_Reward_Avg 9\n",
      " Episode: 133, Episode_Reward: 10, 10_Episode_Reward_Avg 9\n",
      " Episode: 134, Episode_Reward: 8, 10_Episode_Reward_Avg 9\n",
      " Episode: 135, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 136, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 137, Episode_Reward: 10, 10_Episode_Reward_Avg 9\n",
      " Episode: 138, Episode_Reward: 9, 10_Episode_Reward_Avg 9\n",
      " Episode: 139, Episode_Reward: 11, 10_Episode_Reward_Avg 10\n",
      " Episode: 140, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 141, Episode_Reward: 8, 10_Episode_Reward_Avg 9\n",
      " Episode: 142, Episode_Reward: 14, 10_Episode_Reward_Avg 10\n",
      " Episode: 143, Episode_Reward: 19, 10_Episode_Reward_Avg 12\n",
      " Episode: 144, Episode_Reward: 10, 10_Episode_Reward_Avg 12\n",
      " Episode: 145, Episode_Reward: 12, 10_Episode_Reward_Avg 13\n",
      " Episode: 146, Episode_Reward: 11, 10_Episode_Reward_Avg 13\n",
      " Episode: 147, Episode_Reward: 22, 10_Episode_Reward_Avg 15\n",
      " Episode: 148, Episode_Reward: 9, 10_Episode_Reward_Avg 13\n",
      " Episode: 149, Episode_Reward: 12, 10_Episode_Reward_Avg 13\n",
      " Episode: 150, Episode_Reward: 10, 10_Episode_Reward_Avg 13\n",
      " Episode: 151, Episode_Reward: 9, 10_Episode_Reward_Avg 12\n",
      " Episode: 152, Episode_Reward: 34, 10_Episode_Reward_Avg 15\n",
      " Episode: 153, Episode_Reward: 9, 10_Episode_Reward_Avg 15\n",
      " Episode: 154, Episode_Reward: 10, 10_Episode_Reward_Avg 14\n",
      " Episode: 155, Episode_Reward: 43, 10_Episode_Reward_Avg 21\n",
      " Episode: 156, Episode_Reward: 9, 10_Episode_Reward_Avg 21\n",
      " Episode: 157, Episode_Reward: 32, 10_Episode_Reward_Avg 21\n",
      " Episode: 158, Episode_Reward: 17, 10_Episode_Reward_Avg 22\n",
      " Episode: 159, Episode_Reward: 11, 10_Episode_Reward_Avg 22\n",
      " Episode: 160, Episode_Reward: 29, 10_Episode_Reward_Avg 20\n",
      " Episode: 161, Episode_Reward: 9, 10_Episode_Reward_Avg 20\n",
      " Episode: 162, Episode_Reward: 9, 10_Episode_Reward_Avg 15\n",
      " Episode: 163, Episode_Reward: 45, 10_Episode_Reward_Avg 21\n",
      " Episode: 164, Episode_Reward: 9, 10_Episode_Reward_Avg 20\n",
      " Episode: 165, Episode_Reward: 9, 10_Episode_Reward_Avg 16\n",
      " Episode: 166, Episode_Reward: 10, 10_Episode_Reward_Avg 16\n",
      " Episode: 167, Episode_Reward: 9, 10_Episode_Reward_Avg 16\n",
      " Episode: 168, Episode_Reward: 16, 10_Episode_Reward_Avg 11\n",
      " Episode: 169, Episode_Reward: 58, 10_Episode_Reward_Avg 20\n",
      " Episode: 170, Episode_Reward: 12, 10_Episode_Reward_Avg 21\n",
      " Episode: 171, Episode_Reward: 9, 10_Episode_Reward_Avg 21\n",
      " Episode: 172, Episode_Reward: 42, 10_Episode_Reward_Avg 27\n",
      " Episode: 173, Episode_Reward: 60, 10_Episode_Reward_Avg 36\n",
      " Episode: 174, Episode_Reward: 9, 10_Episode_Reward_Avg 26\n",
      " Episode: 175, Episode_Reward: 9, 10_Episode_Reward_Avg 26\n",
      " Episode: 176, Episode_Reward: 10, 10_Episode_Reward_Avg 26\n",
      " Episode: 177, Episode_Reward: 13, 10_Episode_Reward_Avg 20\n",
      " Episode: 178, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 179, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 180, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 181, Episode_Reward: 9, 10_Episode_Reward_Avg 10\n",
      " Episode: 182, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 183, Episode_Reward: 37, 10_Episode_Reward_Avg 15\n",
      " Episode: 184, Episode_Reward: 39, 10_Episode_Reward_Avg 21\n",
      " Episode: 185, Episode_Reward: 48, 10_Episode_Reward_Avg 29\n",
      " Episode: 186, Episode_Reward: 36, 10_Episode_Reward_Avg 34\n",
      " Episode: 187, Episode_Reward: 13, 10_Episode_Reward_Avg 35\n",
      " Episode: 188, Episode_Reward: 44, 10_Episode_Reward_Avg 36\n",
      " Episode: 189, Episode_Reward: 39, 10_Episode_Reward_Avg 36\n",
      " Episode: 190, Episode_Reward: 58, 10_Episode_Reward_Avg 38\n",
      " Episode: 191, Episode_Reward: 43, 10_Episode_Reward_Avg 39\n",
      " Episode: 192, Episode_Reward: 49, 10_Episode_Reward_Avg 47\n",
      " Episode: 193, Episode_Reward: 43, 10_Episode_Reward_Avg 46\n",
      " Episode: 194, Episode_Reward: 45, 10_Episode_Reward_Avg 48\n",
      " Episode: 195, Episode_Reward: 22, 10_Episode_Reward_Avg 40\n",
      " Episode: 196, Episode_Reward: 41, 10_Episode_Reward_Avg 40\n",
      " Episode: 197, Episode_Reward: 40, 10_Episode_Reward_Avg 38\n",
      " Episode: 198, Episode_Reward: 25, 10_Episode_Reward_Avg 35\n",
      " Episode: 199, Episode_Reward: 30, 10_Episode_Reward_Avg 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0,max_episodes):\n",
    "    my_agent.reset_env()\n",
    "    done = False\n",
    "    rewards_episodic = 0\n",
    "    while not(done):\n",
    "        state, action, reward, next_state, done = my_agent.play_one_step()\n",
    "\n",
    "        rewards_episodic += 1\n",
    "        my_agent.training_step()\n",
    "        \n",
    "        \n",
    "    \n",
    "    my_agent.apply_epsilon_decay()\n",
    "    my_agent.update_target_network()\n",
    "    \n",
    "    rewards_buffer.append(round(rewards_episodic,2))\n",
    "    mean_score = round(np.mean(rewards_buffer))\n",
    "    mean_score_buffer.append(rewards_episodic)\n",
    "    recent_mean = round(np.mean(list(mean_score_buffer)))\n",
    "    \n",
    "    print(\"\\r Episode: {}, Episode_Reward: {}, 10_Episode_Reward_Avg {}\".format(i, round(rewards_episodic,2), recent_mean, end=\"\"))\n",
    "    \n",
    "             \n",
    "    if recent_mean >= 200.0:\n",
    "        print('DQN solved problem terminating...')\n",
    "        break\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "     \n",
    "          \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tensorflow]",
   "language": "python",
   "name": "conda-env-Tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
