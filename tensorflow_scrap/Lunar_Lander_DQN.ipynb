{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "import tensorboard\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tensorboard():\n",
    "    import os\n",
    "    tensor_board_sess_path = 'C:/Users/Eaj59/AppData/Local/Temp/.tensorboard-info/'\n",
    "    temp_sess_files = os.listdir(tensor_board_sess_path)\n",
    "    for i in temp_sess_files:\n",
    "        temp_file_path = os.path.join(tensor_board_sess_path,i)\n",
    "        os.remove(temp_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tensorboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 11612."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#clean_tensorboard()\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'C:/Users/Eaj59/Documents/RL_Projects/Project_2_DRL/log_dir/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self,env_name,enable_tb_logging):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_state = self.env.reset()\n",
    "        self.lr = 0.001\n",
    "        self.gamma = .99\n",
    "        self.epsilon_decay_rate = .99\n",
    "        self.initializer = tf.keras.initializers.HeUniform(seed=None)\n",
    "        self.loss_fn = tf.keras.losses.Huber(delta=1.0)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=self.lr)\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.n_states = self.env_state.shape[0]\n",
    "        self.online = self.make_nn()\n",
    "        self.target = self.make_target()\n",
    "        self.buffer = ReplayBuffer(max_len=10000)\n",
    "        self.tau = 0.05\n",
    "        self.batch_size = 256\n",
    "        self.epsilon = .99\n",
    "        self.min_epsilon = .005\n",
    "        self.episode_counter = 0\n",
    "        self.episode_step_counter = 0\n",
    "        self.episode_reward = 0\n",
    "        self.enable_tb_logging = enable_tb_logging\n",
    "        if self.enable_tb_logging:\n",
    "            self.online_run_id, self.online_tensor_board_callback = self.create_tensor_board_callback(model_name='Lunar_Lander_Online_DQN')\n",
    "            self.training_steps_counter = 0\n",
    "        #self.target_run_id, self.target_tensor_board_callback = self.create_tensor_board_callback(model_name='Target_DQN')\n",
    "        \n",
    "        \n",
    "    def generate_run_directory(self,root_log_dir,model_name):\n",
    "        run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "        run_id = model_name+'_'+run_id\n",
    "        base_dir = 'C:/Users/Eaj59/Documents/RL_Projects/Project_2_DRL'\n",
    "        os.chdir('C:/Users/Eaj59/Documents/RL_Projects/Project_2_DRL/log_dir')\n",
    "        os.mkdir(run_id)\n",
    "        os.chdir(base_dir)\n",
    "        return run_id\n",
    "        \n",
    "    \n",
    "    def create_tensor_board_callback(self,model_name):\n",
    "        root_log_dir = os.path.join(os.curdir,'log_dir')\n",
    "        run_id = self.generate_run_directory(root_log_dir=root_log_dir,model_name=model_name)\n",
    "        model_cb_path = os.path.join(root_log_dir,run_id)\n",
    "        file_writer = tf.summary.create_file_writer(model_cb_path)\n",
    "        return run_id, file_writer\n",
    "    \n",
    "    def reset_env_state(self):\n",
    "        self.env_state = self.env.reset()\n",
    "        if self.enable_tb_logging:\n",
    "            with self.online_tensor_board_callback.as_default():\n",
    "                tf.summary.scalar(name='Episode_Time_Steps_Taken',data=self.episode_step_counter,step=self.episode_counter)\n",
    "                tf.summary.scalar(name='Episode Reward',data=self.episode_reward,step=self.episode_counter)\n",
    "        \n",
    "        self.episode_counter += 1\n",
    "        self.episode_step_counter = 0\n",
    "        self.episode_reward = 0\n",
    "        \n",
    "    def make_nn(self):\n",
    "        online = tf.keras.models.Sequential()\n",
    "        online.add(tf.keras.layers.Dense(128,input_dim=self.n_states,activation='swish',kernel_initializer=self.initializer))\n",
    "        online.add(tf.keras.layers.Dense(64, activation='swish',kernel_initializer=self.initializer))\n",
    "        online.add(tf.keras.layers.Dense(32, activation='swish',kernel_initializer=self.initializer))\n",
    "        online.add(tf.keras.layers.Dense(24, activation='swish',kernel_initializer=self.initializer))\n",
    "        online.add(tf.keras.layers.Dense(self.n_actions,activation='linear',kernel_initializer=self.initializer))\n",
    "        online.compile(loss=self.loss_fn,optimizer=self.optimizer,metrics=['accuracy'])\n",
    "        return online\n",
    "        \n",
    "        \n",
    "    def make_target(self):\n",
    "        target = tf.keras.models.clone_model(self.online)\n",
    "        target.set_weights(self.online.get_weights())\n",
    "        return target\n",
    "        \n",
    "        \n",
    "    def epsilon_greedy_policy(self):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return np.random.randint(low=0,high=self.n_actions)\n",
    "        else:\n",
    "            Q_values = self.online.predict(np.reshape(self.env_state,(1,len(self.env_state))))\n",
    "            return np.argmax(Q_values[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def play_one_step(self):\n",
    "        state = np.array(self.env_state)\n",
    "        action = self.epsilon_greedy_policy()\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        self.buffer.add_exp(state=state, action=action, reward=reward, next_state=next_state, done=done)\n",
    "        self.env_state = next_state\n",
    "        self.episode_reward += reward\n",
    "        self.episode_step_counter +=1\n",
    "        return state, action, reward, next_state, done\n",
    "        \n",
    "        \n",
    "    def soft_target_update(self):\n",
    "        target_weights_current = self.target.get_weights()\n",
    "        online_weights_current = self.online.get_weights()\n",
    "        layer_counter = 0\n",
    "        for tw, ow in zip(target_weights_current,online_weights_current):\n",
    "            new_target_weights = (1-self.tau)*tw + (self.tau)*ow\n",
    "            target_weights_current[layer_counter] = new_target_weights\n",
    "            layer_counter += 1\n",
    "        self.target.set_weights(target_weights_current)\n",
    "        \n",
    "        \n",
    "    def training_step(self):\n",
    "        \n",
    "        if len(self.buffer)<self.batch_size:\n",
    "            batch_size_instance = len(self.buffer)\n",
    "        else:\n",
    "            batch_size_instance = self.batch_size\n",
    "            \n",
    "        states,actions,rewards,next_states,dones = self.buffer.exp_sample(batch_size=batch_size_instance)\n",
    "        online_state_prediction = self.online.predict(states)\n",
    "        target_max_next_state_action_value_prediction = np.amax(self.target.predict(next_states),axis=1,keepdims=True)\n",
    "        q_update = rewards + (np.ones(shape=(batch_size_instance,1)) - dones) * self.gamma * target_max_next_state_action_value_prediction\n",
    "\n",
    "        #keep the action not taken the same value only change the action taken value\n",
    "        for i in range(0,online_state_prediction.shape[0]):\n",
    "            if actions[i] == 0:\n",
    "                online_state_prediction[i,0] = q_update[i]\n",
    "            else:\n",
    "                online_state_prediction[i,1] = q_update[i]\n",
    "        \n",
    "        if self.enable_tb_logging:\n",
    "            metrics_output = self.online.train_on_batch(states,online_state_prediction,reset_metrics=True)\n",
    "            self.training_steps_counter+=1\n",
    "            with self.online_tensor_board_callback.as_default():\n",
    "                tf.summary.scalar(name='Batch_MSE_Loss',data=metrics_output[0],step=self.training_steps_counter)\n",
    "                tf.summary.scalar(name='Accuracy',data=metrics_output[1],step=self.training_steps_counter)\n",
    "                tf.summary.scalar(name='Epsilon',data=self.epsilon,step=self.training_steps_counter)\n",
    "                tf.summary.scalar(name='Episode',data=self.episode_counter,step=self.training_steps_counter)\n",
    "                \n",
    "        else:\n",
    "            self.online.train_on_batch(states,online_state_prediction)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    def apply_epsilon_decay(self):\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay_rate,self.min_epsilon)\n",
    "        \n",
    "            \n",
    "    def hard_target_update(self):\n",
    "        self.target.set_weights(self.online.get_weights())\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self,max_len):\n",
    "        self.max_len = max_len\n",
    "        self.buffer = deque(maxlen=self.max_len)\n",
    "        \n",
    "    \n",
    "    def add_exp(self,state,action,reward,next_state,done):\n",
    "        exp = (state,action,reward, next_state, done)\n",
    "        if len(self.buffer)<= self.max_len:\n",
    "            self.buffer.append(exp)\n",
    "        else:\n",
    "            self.buffer[0] = exp\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def exp_sample(self,batch_size):\n",
    "        indices = np.random.randint(len(self.buffer), size=batch_size)\n",
    "        batch = [self.buffer[index] for index in indices]\n",
    "        states, actions, rewards, next_states, dones = [np.array([experience[entry] for experience in batch])for entry in range(5)]\n",
    "        return states, actions, rewards[:,np.newaxis], next_states, dones[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Steps: -100.11179035419424, recent_avg -100\n",
      "Episode: 1, Steps: -163.68413254461555, recent_avg -132\n",
      "Episode: 2, Steps: -32.106811270142416, recent_avg -99\n",
      "Episode: 3, Steps: -18.638402572311193, recent_avg -79\n",
      "Episode: 4, Steps: -144.84745616949522, recent_avg -92\n",
      "Episode: 5, Steps: -275.1049925926842, recent_avg -122\n",
      "Episode: 6, Steps: -152.9917446888412, recent_avg -127\n",
      "Episode: 7, Steps: -205.49918111807148, recent_avg -137\n",
      "Episode: 8, Steps: -103.87257899576888, recent_avg -133\n",
      "Episode: 9, Steps: -128.61569773251213, recent_avg -133\n",
      "Episode: 10, Steps: -202.6781782474799, recent_avg -143\n",
      "Episode: 11, Steps: -110.78005233025328, recent_avg -138\n",
      "Episode: 12, Steps: -229.67611839320517, recent_avg -157\n",
      "Episode: 13, Steps: -126.08622225785221, recent_avg -168\n",
      "Episode: 14, Steps: -74.11508240703837, recent_avg -161\n",
      "Episode: 15, Steps: -108.07836088003924, recent_avg -144\n",
      "Episode: 16, Steps: -132.0526782699666, recent_avg -142\n",
      "Episode: 17, Steps: -126.91165708089086, recent_avg -134\n",
      "Episode: 18, Steps: -22.184021726585016, recent_avg -126\n",
      "Episode: 19, Steps: -236.17364604890162, recent_avg -137\n",
      "Episode: 20, Steps: -315.1146892626337, recent_avg -148\n",
      "Episode: 21, Steps: -384.1014490013138, recent_avg -175\n",
      "Episode: 22, Steps: -291.66493480060865, recent_avg -182\n",
      "Episode: 23, Steps: -92.85602147411007, recent_avg -178\n",
      "Episode: 24, Steps: -228.78052973095876, recent_avg -194\n",
      "Episode: 25, Steps: -340.60667557784376, recent_avg -217\n",
      "Episode: 26, Steps: 0.9458272718965759, recent_avg -204\n",
      "Episode: 27, Steps: -435.61495787998047, recent_avg -235\n",
      "Episode: 28, Steps: -117.47047122206737, recent_avg -244\n",
      "Episode: 29, Steps: -271.35800164436, recent_avg -248\n",
      "Episode: 30, Steps: -359.15691504226936, recent_avg -252\n",
      "Episode: 31, Steps: -25.54639635806167, recent_avg -216\n",
      "Episode: 32, Steps: -114.60980791291105, recent_avg -199\n",
      "Episode: 33, Steps: -317.4912130490162, recent_avg -221\n",
      "Episode: 34, Steps: -268.1706046787677, recent_avg -225\n",
      "Episode: 35, Steps: -241.04370797639376, recent_avg -215\n",
      "Episode: 36, Steps: -73.93601867573283, recent_avg -222\n",
      "Episode: 37, Steps: -404.9147775303134, recent_avg -219\n",
      "Episode: 38, Steps: -140.22363540938682, recent_avg -222\n",
      "Episode: 39, Steps: -363.77444803896213, recent_avg -231\n",
      "Episode: 40, Steps: -404.87286495360064, recent_avg -235\n",
      "Episode: 41, Steps: -84.13850054746561, recent_avg -241\n",
      "Episode: 42, Steps: -151.58526000918425, recent_avg -245\n",
      "Episode: 43, Steps: -260.0821135515513, recent_avg -239\n",
      "Episode: 44, Steps: -126.25614322018752, recent_avg -225\n",
      "Episode: 45, Steps: -338.37795963757856, recent_avg -235\n",
      "Episode: 46, Steps: -297.8429557731297, recent_avg -257\n",
      "Episode: 47, Steps: -144.81185920622565, recent_avg -231\n",
      "Episode: 48, Steps: -435.6103059010975, recent_avg -261\n",
      "Episode: 49, Steps: -70.01340917374247, recent_avg -231\n",
      "Episode: 50, Steps: -325.5163564822866, recent_avg -223\n",
      "Episode: 51, Steps: -391.62753517206323, recent_avg -254\n",
      "Episode: 52, Steps: -159.56587425184088, recent_avg -255\n",
      "Episode: 53, Steps: -333.6379110069472, recent_avg -262\n",
      "Episode: 54, Steps: -101.97042497151119, recent_avg -260\n",
      "Episode: 55, Steps: -462.1886979443807, recent_avg -272\n",
      "Episode: 56, Steps: -203.39010177544117, recent_avg -263\n",
      "Episode: 57, Steps: -143.06978964076208, recent_avg -263\n",
      "Episode: 58, Steps: -129.57972494958503, recent_avg -232\n",
      "Episode: 59, Steps: -120.04296831173677, recent_avg -237\n",
      "Episode: 60, Steps: -142.2894692438475, recent_avg -219\n",
      "Episode: 61, Steps: -150.73286290073003, recent_avg -195\n",
      "Episode: 62, Steps: -118.0395348141697, recent_avg -190\n",
      "Episode: 63, Steps: -154.56610523638636, recent_avg -173\n",
      "Episode: 64, Steps: -78.57624801666299, recent_avg -170\n",
      "Episode: 65, Steps: -129.07169037011377, recent_avg -137\n",
      "Episode: 66, Steps: -124.34013548471442, recent_avg -129\n",
      "Episode: 67, Steps: 13.096859830164405, recent_avg -113\n",
      "Episode: 68, Steps: -139.52376044121502, recent_avg -114\n",
      "Episode: 69, Steps: -197.26648086421324, recent_avg -122\n",
      "Episode: 70, Steps: -346.9120662755626, recent_avg -143\n",
      "Episode: 71, Steps: -526.0404570029561, recent_avg -180\n",
      "Episode: 72, Steps: -97.03980028838254, recent_avg -178\n",
      "Episode: 73, Steps: -138.3236122421132, recent_avg -176\n",
      "Episode: 74, Steps: -13.777891133895196, recent_avg -170\n",
      "Episode: 75, Steps: -131.5460949856733, recent_avg -170\n",
      "Episode: 76, Steps: -151.13111682121146, recent_avg -173\n",
      "Episode: 77, Steps: -132.9953442513806, recent_avg -187\n",
      "Episode: 78, Steps: -563.5906146521411, recent_avg -230\n",
      "Episode: 79, Steps: -107.55208198750864, recent_avg -221\n",
      "Episode: 80, Steps: -171.23472103236824, recent_avg -203\n",
      "Episode: 81, Steps: -336.6294936261322, recent_avg -184\n",
      "Episode: 82, Steps: -190.84067755235037, recent_avg -194\n",
      "Episode: 83, Steps: -153.17398133159085, recent_avg -195\n",
      "Episode: 84, Steps: -387.2074940339808, recent_avg -233\n",
      "Episode: 85, Steps: -135.24813213294297, recent_avg -233\n",
      "Episode: 86, Steps: -125.57762723632618, recent_avg -230\n",
      "Episode: 87, Steps: -335.62167679421873, recent_avg -251\n",
      "Episode: 88, Steps: -158.8088084025408, recent_avg -210\n",
      "Episode: 89, Steps: -118.00282424331525, recent_avg -211\n",
      "Episode: 90, Steps: -104.90867499071695, recent_avg -205\n",
      "Episode: 91, Steps: -292.2387007932972, recent_avg -200\n",
      "Episode: 92, Steps: -189.97503079418576, recent_avg -200\n",
      "Episode: 93, Steps: -165.67283020377658, recent_avg -201\n",
      "Episode: 94, Steps: -323.145396768673, recent_avg -195\n",
      "Episode: 95, Steps: -269.35746150226265, recent_avg -208\n",
      "Episode: 96, Steps: -123.02951590310853, recent_avg -208\n",
      "Episode: 97, Steps: -186.11299308991778, recent_avg -193\n",
      "Episode: 98, Steps: -294.24247632995593, recent_avg -207\n",
      "Episode: 99, Steps: -175.81216333511006, recent_avg -212\n",
      "Episode: 100, Steps: -196.55664801972853, recent_avg -222\n",
      "Episode: 101, Steps: -238.14373055824277, recent_avg -216\n",
      "Episode: 102, Steps: -122.92988018462333, recent_avg -210\n",
      "Episode: 103, Steps: -126.2270851001505, recent_avg -206\n",
      "Episode: 104, Steps: -182.44000627937635, recent_avg -191\n",
      "Episode: 105, Steps: -110.88026525202241, recent_avg -176\n",
      "Episode: 106, Steps: -100.60152438758864, recent_avg -173\n",
      "Episode: 107, Steps: -131.62610866004755, recent_avg -168\n",
      "Episode: 108, Steps: -253.84590682348335, recent_avg -164\n",
      "Episode: 109, Steps: -184.6109981408003, recent_avg -165\n",
      "Episode: 110, Steps: 16.111806692651868, recent_avg -144\n",
      "Episode: 111, Steps: -238.24205226366237, recent_avg -144\n",
      "Episode: 112, Steps: -95.3506046019103, recent_avg -141\n",
      "Episode: 113, Steps: -244.45363850089348, recent_avg -153\n",
      "Episode: 114, Steps: -184.172712001833, recent_avg -153\n",
      "Episode: 115, Steps: -265.7151616190888, recent_avg -168\n",
      "Episode: 116, Steps: -150.86088772548175, recent_avg -173\n",
      "Episode: 117, Steps: -100.2401054904535, recent_avg -170\n",
      "Episode: 118, Steps: -5.689970796681351, recent_avg -145\n",
      "Episode: 119, Steps: -290.2566341175085, recent_avg -156\n",
      "Episode: 120, Steps: -244.05832402793084, recent_avg -182\n",
      "Episode: 121, Steps: -95.66121139788538, recent_avg -168\n",
      "Episode: 122, Steps: -36.65876505228889, recent_avg -162\n",
      "Episode: 123, Steps: -100.84198780353498, recent_avg -147\n",
      "Episode: 124, Steps: -131.24940178951786, recent_avg -142\n",
      "Episode: 125, Steps: -95.08898836302353, recent_avg -125\n",
      "Episode: 126, Steps: -277.8024432019334, recent_avg -138\n",
      "Episode: 127, Steps: -190.35733993345843, recent_avg -147\n",
      "Episode: 128, Steps: -112.92732730291354, recent_avg -157\n",
      "Episode: 129, Steps: -135.1826832658593, recent_avg -142\n",
      "Episode: 130, Steps: -119.21304988358975, recent_avg -129\n",
      "Episode: 131, Steps: -123.9022284932943, recent_avg -132\n",
      "Episode: 132, Steps: -119.89321465704867, recent_avg -141\n",
      "Episode: 133, Steps: -144.6959612764563, recent_avg -145\n",
      "Episode: 134, Steps: -103.19422734366864, recent_avg -142\n",
      "Episode: 135, Steps: -171.7627669864167, recent_avg -150\n",
      "Episode: 136, Steps: -151.89874887594144, recent_avg -137\n",
      "Episode: 137, Steps: -113.76314304408717, recent_avg -130\n",
      "Episode: 138, Steps: -170.70249561268344, recent_avg -135\n",
      "Episode: 139, Steps: -148.56986026015355, recent_avg -137\n",
      "Episode: 140, Steps: -110.621933282491, recent_avg -136\n",
      "Episode: 141, Steps: -109.67656876325296, recent_avg -134\n",
      "Episode: 142, Steps: -229.3275589811966, recent_avg -145\n",
      "Episode: 143, Steps: -224.52648924074077, recent_avg -153\n",
      "Episode: 144, Steps: -134.92759424283366, recent_avg -157\n",
      "Episode: 145, Steps: -149.94598052125752, recent_avg -154\n",
      "Episode: 146, Steps: -324.413207216295, recent_avg -172\n",
      "Episode: 147, Steps: -155.43068883368602, recent_avg -176\n",
      "Episode: 148, Steps: -134.75897470824378, recent_avg -172\n",
      "Episode: 149, Steps: -156.6350020053959, recent_avg -173\n",
      "Episode: 150, Steps: -140.36555368994917, recent_avg -176\n",
      "Episode: 151, Steps: -101.13572140170405, recent_avg -175\n",
      "Episode: 152, Steps: -116.9361260136179, recent_avg -164\n",
      "Episode: 153, Steps: -281.35230120545526, recent_avg -170\n",
      "Episode: 154, Steps: -111.43451018965933, recent_avg -167\n",
      "Episode: 155, Steps: -224.80012165908948, recent_avg -175\n",
      "Episode: 156, Steps: -157.61145136610173, recent_avg -158\n",
      "Episode: 157, Steps: -130.42918167461983, recent_avg -156\n",
      "Episode: 158, Steps: -105.4562628254895, recent_avg -153\n",
      "Episode: 159, Steps: -109.33039055021547, recent_avg -148\n",
      "Episode: 160, Steps: -109.34129400811531, recent_avg -145\n",
      "Episode: 161, Steps: -108.44633400201441, recent_avg -146\n",
      "Episode: 162, Steps: 35.34954104367691, recent_avg -130\n",
      "Episode: 163, Steps: -154.49037572758368, recent_avg -118\n",
      "Episode: 164, Steps: -247.70376845757028, recent_avg -131\n",
      "Episode: 165, Steps: -115.21540970887153, recent_avg -120\n",
      "Episode: 166, Steps: -96.43762302511753, recent_avg -114\n",
      "Episode: 167, Steps: -113.65321528927731, recent_avg -112\n",
      "Episode: 168, Steps: -232.85036158032406, recent_avg -125\n",
      "Episode: 169, Steps: -90.02341127136599, recent_avg -123\n",
      "Episode: 170, Steps: -142.18331417043692, recent_avg -127\n",
      "Episode: 171, Steps: -111.12790930469455, recent_avg -127\n",
      "Episode: 172, Steps: -107.96671355324058, recent_avg -141\n",
      "Episode: 173, Steps: -140.16688074673843, recent_avg -140\n",
      "Episode: 174, Steps: -166.976894027358, recent_avg -132\n",
      "Episode: 175, Steps: -124.73400168316181, recent_avg -133\n",
      "Episode: 176, Steps: -93.68657055723126, recent_avg -132\n",
      "Episode: 177, Steps: -218.66436917821946, recent_avg -143\n",
      "Episode: 178, Steps: -115.21626007237637, recent_avg -131\n",
      "Episode: 179, Steps: -170.62690877306548, recent_avg -139\n",
      "Episode: 180, Steps: -149.53194085429993, recent_avg -140\n",
      "Episode: 181, Steps: -94.60737864790948, recent_avg -138\n",
      "Episode: 182, Steps: -172.82879128442485, recent_avg -145\n",
      "Episode: 183, Steps: -138.90059541944416, recent_avg -145\n",
      "Episode: 184, Steps: -118.37900091939358, recent_avg -140\n",
      "Episode: 185, Steps: -81.93318830045962, recent_avg -135\n",
      "Episode: 186, Steps: -44.70025517320809, recent_avg -131\n",
      "Episode: 187, Steps: -130.7966347024616, recent_avg -122\n",
      "Episode: 188, Steps: -141.78937791889533, recent_avg -124\n",
      "Episode: 189, Steps: 16.828214922487135, recent_avg -106\n",
      "Episode: 190, Steps: -139.15620529721025, recent_avg -105\n",
      "Episode: 191, Steps: -166.85133227459727, recent_avg -112\n",
      "Episode: 192, Steps: -138.08539694498467, recent_avg -108\n",
      "Episode: 193, Steps: -182.78526670509171, recent_avg -113\n",
      "Episode: 194, Steps: -135.59325795404055, recent_avg -114\n",
      "Episode: 195, Steps: -163.45178219366124, recent_avg -123\n",
      "Episode: 196, Steps: -15.96319178843973, recent_avg -120\n",
      "Episode: 197, Steps: -166.27323708799466, recent_avg -123\n",
      "Episode: 198, Steps: -360.5877401026812, recent_avg -145\n",
      "Episode: 199, Steps: -200.08391968072232, recent_avg -167\n",
      "Episode: 200, Steps: -166.08317859835336, recent_avg -170\n",
      "Episode: 201, Steps: -150.25440950482954, recent_avg -168\n",
      "Episode: 202, Steps: -150.98794924319327, recent_avg -169\n",
      "Episode: 203, Steps: -88.63878462116145, recent_avg -160\n",
      "Episode: 204, Steps: -110.05812494980819, recent_avg -157\n",
      "Episode: 205, Steps: -124.50143876915095, recent_avg -153\n",
      "Episode: 206, Steps: -123.13125781484904, recent_avg -164\n",
      "Episode: 207, Steps: -120.2741205897612, recent_avg -159\n",
      "Episode: 208, Steps: -156.83414446387144, recent_avg -139\n",
      "Episode: 209, Steps: -0.20812118573111604, recent_avg -119\n",
      "Episode: 210, Steps: -127.51723361670174, recent_avg -115\n",
      "Episode: 211, Steps: -106.22025889160582, recent_avg -111\n",
      "Episode: 212, Steps: -14.84222154180469, recent_avg -97\n",
      "Episode: 213, Steps: -248.4051466882515, recent_avg -113\n",
      "Episode: 214, Steps: -142.3763429433918, recent_avg -116\n",
      "Episode: 215, Steps: -146.74331521201344, recent_avg -119\n",
      "Episode: 216, Steps: -162.118367447509, recent_avg -123\n",
      "Episode: 217, Steps: -98.6552242030289, recent_avg -120\n",
      "Episode: 218, Steps: -147.29559344401395, recent_avg -119\n",
      "Episode: 219, Steps: -94.47088926700837, recent_avg -129\n",
      "Episode: 220, Steps: -145.5746639937887, recent_avg -131\n",
      "Episode: 221, Steps: -145.81142443453476, recent_avg -135\n",
      "Episode: 222, Steps: -118.46202617151964, recent_avg -145\n",
      "Episode: 223, Steps: -34.76902783553929, recent_avg -124\n",
      "Episode: 224, Steps: -121.52322220973029, recent_avg -122\n",
      "Episode: 225, Steps: -99.01650983787722, recent_avg -117\n",
      "Episode: 226, Steps: -306.300867663806, recent_avg -131\n",
      "Episode: 227, Steps: -135.50965216034632, recent_avg -135\n",
      "Episode: 228, Steps: -162.66877530672843, recent_avg -136\n",
      "Episode: 229, Steps: -209.10466373040566, recent_avg -148\n",
      "Episode: 230, Steps: -156.32041715623149, recent_avg -149\n",
      "Episode: 231, Steps: -114.78920320979299, recent_avg -146\n",
      "Episode: 232, Steps: -170.96260661552066, recent_avg -151\n",
      "Episode: 233, Steps: -150.7174766115538, recent_avg -163\n",
      "Episode: 234, Steps: -126.53537061854945, recent_avg -163\n",
      "Episode: 235, Steps: -175.73380878293804, recent_avg -171\n",
      "Episode: 236, Steps: -178.51424283130746, recent_avg -158\n",
      "Episode: 237, Steps: -142.5327322474974, recent_avg -159\n",
      "Episode: 238, Steps: -127.50241754502584, recent_avg -155\n",
      "Episode: 239, Steps: -110.22089750689368, recent_avg -145\n",
      "Episode: 240, Steps: -120.7906537563916, recent_avg -142\n",
      "Episode: 241, Steps: -136.43404944085907, recent_avg -144\n",
      "Episode: 242, Steps: -172.07593684990445, recent_avg -144\n",
      "Episode: 243, Steps: -133.3326524507096, recent_avg -142\n",
      "Episode: 244, Steps: -100.10870127367873, recent_avg -140\n",
      "Episode: 245, Steps: -230.8551751349394, recent_avg -145\n",
      "Episode: 246, Steps: -133.55423954954625, recent_avg -141\n",
      "Episode: 247, Steps: -118.11974302497968, recent_avg -138\n",
      "Episode: 248, Steps: -172.6405540627594, recent_avg -143\n",
      "Episode: 249, Steps: -161.01336754559662, recent_avg -148\n",
      "Episode: 250, Steps: -98.93111387138067, recent_avg -146\n",
      "Episode: 251, Steps: -127.43674200730607, recent_avg -145\n",
      "Episode: 252, Steps: -129.86263153140925, recent_avg -141\n",
      "Episode: 253, Steps: -102.64832450977855, recent_avg -138\n",
      "Episode: 254, Steps: -144.7479600120626, recent_avg -142\n",
      "Episode: 255, Steps: -124.56630070803395, recent_avg -131\n",
      "Episode: 256, Steps: -155.35098487167727, recent_avg -134\n",
      "Episode: 257, Steps: -144.2271609195791, recent_avg -136\n",
      "Episode: 258, Steps: -2.9568778526807336, recent_avg -119\n",
      "Episode: 259, Steps: -106.6955743571761, recent_avg -114\n",
      "Episode: 260, Steps: -145.81015142730655, recent_avg -118\n",
      "Episode: 261, Steps: -93.44353338550714, recent_avg -115\n",
      "Episode: 262, Steps: -163.92523616292232, recent_avg -118\n",
      "Episode: 263, Steps: -118.76537154338308, recent_avg -120\n",
      "Episode: 264, Steps: -167.0715642194217, recent_avg -122\n",
      "Episode: 265, Steps: -137.8141699136218, recent_avg -124\n",
      "Episode: 266, Steps: -15.040606635354791, recent_avg -110\n",
      "Episode: 267, Steps: -199.94716579102277, recent_avg -115\n",
      "Episode: 268, Steps: -155.26911518683997, recent_avg -130\n",
      "Episode: 269, Steps: -119.53276763100816, recent_avg -132\n",
      "Episode: 270, Steps: -183.3469445875644, recent_avg -135\n",
      "Episode: 271, Steps: -146.13802180523547, recent_avg -141\n",
      "Episode: 272, Steps: -156.4614561743998, recent_avg -140\n",
      "Episode: 273, Steps: -132.8211792381719, recent_avg -141\n",
      "Episode: 274, Steps: -129.5289251863827, recent_avg -138\n",
      "Episode: 275, Steps: -124.8191447932135, recent_avg -136\n",
      "Episode: 276, Steps: -142.1552938529687, recent_avg -149\n",
      "Episode: 277, Steps: -135.58940732372497, recent_avg -143\n",
      "Episode: 278, Steps: -146.5003465788821, recent_avg -142\n",
      "Episode: 279, Steps: -168.0250973047386, recent_avg -147\n",
      "Episode: 280, Steps: -125.14778224734118, recent_avg -141\n",
      "Episode: 281, Steps: -132.3131665706054, recent_avg -139\n",
      "Episode: 282, Steps: -129.4656065891499, recent_avg -137\n",
      "Episode: 283, Steps: -98.28492072114106, recent_avg -133\n",
      "Episode: 284, Steps: -125.61355329030633, recent_avg -133\n",
      "Episode: 285, Steps: -127.26785079594902, recent_avg -133\n",
      "Episode: 286, Steps: -146.7188671048038, recent_avg -133\n",
      "Episode: 287, Steps: -103.40460200862488, recent_avg -130\n",
      "Episode: 288, Steps: -136.91367503465858, recent_avg -129\n",
      "Episode: 289, Steps: -134.02237187146153, recent_avg -126\n",
      "Episode: 290, Steps: -177.44724613394573, recent_avg -131\n",
      "Episode: 291, Steps: -141.52528821566324, recent_avg -132\n",
      "Episode: 292, Steps: -177.23313863415032, recent_avg -137\n",
      "Episode: 293, Steps: -81.31971456560862, recent_avg -135\n",
      "Episode: 294, Steps: -201.32489755510386, recent_avg -143\n",
      "Episode: 295, Steps: -143.55609309483603, recent_avg -144\n",
      "Episode: 296, Steps: -130.97280896390055, recent_avg -143\n",
      "Episode: 297, Steps: -127.95682014282039, recent_avg -145\n",
      "Episode: 298, Steps: -74.15810938258252, recent_avg -139\n",
      "Episode: 299, Steps: -126.69897241101222, recent_avg -138\n",
      "Episode: 300, Steps: -219.31646763576504, recent_avg -142\n",
      "Episode: 301, Steps: -159.81454905941308, recent_avg -144\n",
      "Episode: 302, Steps: -176.93359264049872, recent_avg -144\n",
      "Episode: 303, Steps: -146.5706317040641, recent_avg -151\n",
      "Episode: 304, Steps: -136.81269601118527, recent_avg -144\n",
      "Episode: 305, Steps: -146.47782186025364, recent_avg -145\n",
      "Episode: 306, Steps: -141.65977534910402, recent_avg -146\n",
      "Episode: 307, Steps: -181.87852919021446, recent_avg -151\n",
      "Episode: 308, Steps: -95.47599798741085, recent_avg -153\n",
      "Episode: 309, Steps: -144.7063218120709, recent_avg -155\n",
      "Episode: 310, Steps: -229.15009150610658, recent_avg -156\n",
      "Episode: 311, Steps: -155.64735513363013, recent_avg -156\n",
      "Episode: 312, Steps: -123.84920758005532, recent_avg -150\n",
      "Episode: 313, Steps: -111.08296974297545, recent_avg -147\n",
      "Episode: 314, Steps: -117.11119107063743, recent_avg -145\n",
      "Episode: 315, Steps: -208.0026789248618, recent_avg -151\n",
      "Episode: 316, Steps: -127.66745794636222, recent_avg -149\n",
      "Episode: 317, Steps: -122.29358465927535, recent_avg -143\n",
      "Episode: 318, Steps: -66.18835642980808, recent_avg -141\n",
      "Episode: 319, Steps: -150.76887457918397, recent_avg -141\n",
      "Episode: 320, Steps: -149.04408241823586, recent_avg -133\n",
      "Episode: 321, Steps: -199.5000123452284, recent_avg -138\n",
      "Episode: 322, Steps: -164.51587060470183, recent_avg -142\n",
      "Episode: 323, Steps: -185.9669213743145, recent_avg -149\n",
      "Episode: 324, Steps: -122.5127382487894, recent_avg -150\n",
      "Episode: 325, Steps: -158.22044136478235, recent_avg -145\n",
      "Episode: 326, Steps: -149.5308813938471, recent_avg -147\n",
      "Episode: 327, Steps: -200.67087429185972, recent_avg -155\n",
      "Episode: 328, Steps: -171.17496641336186, recent_avg -165\n",
      "Episode: 329, Steps: -101.77083299009713, recent_avg -160\n",
      "Episode: 330, Steps: -150.13832105569313, recent_avg -160\n",
      "Episode: 331, Steps: -145.0523869928282, recent_avg -155\n",
      "Episode: 332, Steps: -116.89392204361661, recent_avg -150\n",
      "Episode: 333, Steps: -145.74622049632842, recent_avg -146\n",
      "Episode: 334, Steps: -128.5915983346468, recent_avg -147\n",
      "Episode: 335, Steps: -120.74770907130163, recent_avg -143\n",
      "Episode: 336, Steps: -176.23794019391403, recent_avg -146\n",
      "Episode: 337, Steps: -147.71405833210963, recent_avg -140\n",
      "Episode: 338, Steps: -159.81754019372266, recent_avg -139\n",
      "Episode: 339, Steps: -20.921126157698808, recent_avg -131\n",
      "Episode: 340, Steps: -65.5761327225025, recent_avg -123\n",
      "Episode: 341, Steps: -132.95261613121758, recent_avg -122\n",
      "Episode: 342, Steps: -150.6595100361632, recent_avg -125\n",
      "Episode: 343, Steps: 36.06026235167522, recent_avg -107\n",
      "Episode: 344, Steps: -117.87189738100454, recent_avg -106\n",
      "Episode: 345, Steps: -144.96615948199684, recent_avg -108\n",
      "Episode: 346, Steps: -126.61761192587943, recent_avg -103\n",
      "Episode: 347, Steps: -123.43505213871569, recent_avg -101\n",
      "Episode: 348, Steps: -150.60393364746534, recent_avg -100\n",
      "Episode: 349, Steps: -127.28875499201915, recent_avg -110\n",
      "Episode: 350, Steps: -246.15138341815978, recent_avg -128\n",
      "Episode: 351, Steps: -144.4243839040857, recent_avg -130\n",
      "Episode: 352, Steps: -124.09347404683407, recent_avg -127\n",
      "Episode: 353, Steps: -172.88448697393915, recent_avg -148\n",
      "Episode: 354, Steps: -160.8630843911949, recent_avg -152\n",
      "Episode: 355, Steps: -114.55679987395092, recent_avg -149\n",
      "Episode: 356, Steps: -268.83922203022564, recent_avg -163\n",
      "Episode: 357, Steps: -143.25440407629384, recent_avg -165\n",
      "Episode: 358, Steps: -381.03973278774237, recent_avg -188\n",
      "Episode: 359, Steps: -143.83621557741765, recent_avg -190\n",
      "Episode: 360, Steps: -122.83763900694896, recent_avg -178\n",
      "Episode: 361, Steps: -116.56310048777134, recent_avg -175\n",
      "Episode: 362, Steps: -136.56559704505747, recent_avg -176\n",
      "Episode: 363, Steps: -177.1123822087131, recent_avg -177\n",
      "Episode: 364, Steps: -350.0559783601053, recent_avg -195\n",
      "Episode: 365, Steps: -206.38749831985723, recent_avg -205\n",
      "Episode: 366, Steps: -210.18682425958949, recent_avg -199\n",
      "Episode: 367, Steps: -135.33303588721057, recent_avg -198\n",
      "Episode: 368, Steps: -114.55079665544558, recent_avg -171\n",
      "Episode: 369, Steps: -152.38909566371012, recent_avg -172\n",
      "Episode: 370, Steps: -287.41462729343596, recent_avg -189\n",
      "Episode: 371, Steps: -111.11929420580044, recent_avg -188\n",
      "Episode: 372, Steps: -156.41541266176543, recent_avg -190\n",
      "Episode: 373, Steps: -132.2560479659154, recent_avg -186\n",
      "Episode: 374, Steps: -148.6242076269964, recent_avg -165\n",
      "Episode: 375, Steps: -132.94605357555338, recent_avg -158\n",
      "Episode: 376, Steps: -132.99153317393274, recent_avg -150\n",
      "Episode: 377, Steps: -156.6851549613079, recent_avg -153\n",
      "Episode: 378, Steps: -89.49984778178977, recent_avg -150\n",
      "Episode: 379, Steps: -130.44688527991255, recent_avg -148\n",
      "Episode: 380, Steps: -135.206797680332, recent_avg -133\n",
      "Episode: 381, Steps: -113.64524789955092, recent_avg -133\n",
      "Episode: 382, Steps: -143.8707619276219, recent_avg -132\n",
      "Episode: 383, Steps: -100.44625093002196, recent_avg -128\n",
      "Episode: 384, Steps: -100.46755763740256, recent_avg -124\n",
      "Episode: 385, Steps: -170.26797065142244, recent_avg -127\n",
      "Episode: 386, Steps: -176.72598751637685, recent_avg -132\n",
      "Episode: 387, Steps: -148.17834890983858, recent_avg -131\n",
      "Episode: 388, Steps: -166.1299790513716, recent_avg -139\n",
      "Episode: 389, Steps: -351.37902445346197, recent_avg -161\n",
      "Episode: 390, Steps: -167.16001960530068, recent_avg -164\n",
      "Episode: 391, Steps: -41.76561894356669, recent_avg -157\n",
      "Episode: 392, Steps: -140.3424195073785, recent_avg -156\n",
      "Episode: 393, Steps: -107.03155138353266, recent_avg -157\n",
      "Episode: 394, Steps: -172.19252675316883, recent_avg -164\n",
      "Episode: 395, Steps: -125.37739689238342, recent_avg -160\n",
      "Episode: 396, Steps: -126.43965372929038, recent_avg -155\n",
      "Episode: 397, Steps: -140.5382376886822, recent_avg -154\n",
      "Episode: 398, Steps: -113.8863772769642, recent_avg -149\n",
      "Episode: 399, Steps: -117.84621842751227, recent_avg -125\n",
      "Episode: 400, Steps: -193.16433329148143, recent_avg -128\n",
      "Episode: 401, Steps: -145.98479663785375, recent_avg -138\n",
      "Episode: 402, Steps: -111.56951327584446, recent_avg -135\n",
      "Episode: 403, Steps: -344.8756504510115, recent_avg -159\n",
      "Episode: 404, Steps: -174.519869572216, recent_avg -159\n",
      "Episode: 405, Steps: -142.8476992376702, recent_avg -161\n",
      "Episode: 406, Steps: -107.9970637843362, recent_avg -159\n",
      "Episode: 407, Steps: -418.43853080599337, recent_avg -187\n",
      "Episode: 408, Steps: -96.58466269533196, recent_avg -185\n",
      "Episode: 409, Steps: -45.20431695306645, recent_avg -178\n",
      "Episode: 410, Steps: -113.91394848062525, recent_avg -170\n",
      "Episode: 411, Steps: -298.29039334983247, recent_avg -185\n",
      "Episode: 412, Steps: -190.922224204523, recent_avg -193\n",
      "Episode: 413, Steps: -134.4968444985011, recent_avg -172\n",
      "Episode: 414, Steps: -159.83855414879812, recent_avg -171\n",
      "Episode: 415, Steps: -123.27673430946368, recent_avg -169\n",
      "Episode: 416, Steps: -281.8610916018923, recent_avg -186\n",
      "Episode: 417, Steps: -126.04926630660154, recent_avg -157\n",
      "Episode: 418, Steps: -132.03324704851295, recent_avg -161\n",
      "Episode: 419, Steps: -123.31546669767786, recent_avg -168\n",
      "Episode: 420, Steps: -138.76639661031888, recent_avg -171\n",
      "Episode: 421, Steps: -169.50132015387902, recent_avg -158\n",
      "Episode: 422, Steps: -66.10102302894617, recent_avg -146\n",
      "Episode: 423, Steps: -277.86442253766563, recent_avg -160\n",
      "Episode: 424, Steps: -145.9116165530709, recent_avg -158\n",
      "Episode: 425, Steps: -621.2240672127353, recent_avg -208\n",
      "Episode: 426, Steps: -131.84088371968102, recent_avg -193\n",
      "Episode: 427, Steps: -148.53163901286445, recent_avg -196\n",
      "Episode: 428, Steps: -118.73723492409592, recent_avg -194\n",
      "Episode: 429, Steps: -115.16941252924954, recent_avg -193\n",
      "Episode: 430, Steps: -366.1567954173635, recent_avg -216\n",
      "Episode: 431, Steps: -172.3209742516965, recent_avg -216\n",
      "Episode: 432, Steps: -150.19834861501678, recent_avg -225\n",
      "Episode: 433, Steps: -122.3392222771921, recent_avg -209\n",
      "Episode: 434, Steps: -131.6475655134582, recent_avg -208\n",
      "Episode: 435, Steps: -125.01476463963198, recent_avg -158\n",
      "Episode: 436, Steps: -145.0180399021044, recent_avg -160\n",
      "Episode: 437, Steps: -136.77057384215763, recent_avg -158\n",
      "Episode: 438, Steps: -133.5220464990535, recent_avg -160\n",
      "Episode: 439, Steps: -117.31750315509451, recent_avg -160\n",
      "Episode: 440, Steps: -116.65471797072384, recent_avg -135\n",
      "Episode: 441, Steps: -96.69180338273557, recent_avg -128\n",
      "Episode: 442, Steps: -151.20369615561268, recent_avg -128\n",
      "Episode: 443, Steps: -130.0251687711777, recent_avg -128\n",
      "Episode: 444, Steps: -4.801344217446754, recent_avg -116\n",
      "Episode: 445, Steps: -120.90461158201798, recent_avg -115\n",
      "Episode: 446, Steps: -230.85098511733355, recent_avg -124\n",
      "Episode: 447, Steps: -95.36268914101194, recent_avg -120\n",
      "Episode: 448, Steps: -127.95409967135535, recent_avg -119\n",
      "Episode: 449, Steps: -155.17508766441503, recent_avg -123\n",
      "Episode: 450, Steps: -119.09271992741799, recent_avg -123\n",
      "Episode: 451, Steps: -108.79485457743671, recent_avg -124\n",
      "Episode: 452, Steps: -142.30144040893805, recent_avg -124\n",
      "Episode: 453, Steps: -174.55186650783074, recent_avg -128\n",
      "Episode: 454, Steps: -131.51665971521234, recent_avg -141\n",
      "Episode: 455, Steps: -153.69780551931154, recent_avg -144\n",
      "Episode: 456, Steps: -140.9679760097822, recent_avg -135\n",
      "Episode: 457, Steps: -260.01494533482673, recent_avg -151\n",
      "Episode: 458, Steps: -145.94884171163432, recent_avg -153\n",
      "Episode: 459, Steps: -131.76742606672832, recent_avg -151\n",
      "Episode: 460, Steps: -118.13411396328343, recent_avg -151\n",
      "Episode: 461, Steps: -222.73219436867788, recent_avg -162\n",
      "Episode: 462, Steps: -213.06834792512157, recent_avg -169\n",
      "Episode: 463, Steps: -110.9396327375856, recent_avg -163\n",
      "Episode: 464, Steps: -107.69933913947607, recent_avg -160\n",
      "Episode: 465, Steps: -123.82527645064161, recent_avg -158\n",
      "Episode: 466, Steps: -255.50520069002113, recent_avg -169\n",
      "Episode: 467, Steps: -343.12456554411256, recent_avg -177\n",
      "Episode: 468, Steps: -153.43487000085756, recent_avg -178\n",
      "Episode: 469, Steps: -432.8765169174723, recent_avg -208\n",
      "Episode: 470, Steps: -388.5062396232238, recent_avg -235\n",
      "Episode: 471, Steps: -157.6794604498114, recent_avg -229\n",
      "Episode: 472, Steps: -216.81607241946438, recent_avg -229\n",
      "Episode: 473, Steps: -359.0679322316413, recent_avg -254\n",
      "Episode: 474, Steps: -135.77719177760406, recent_avg -257\n",
      "Episode: 475, Steps: -134.19454203133165, recent_avg -258\n",
      "Episode: 476, Steps: -146.22599065447426, recent_avg -247\n",
      "Episode: 477, Steps: -132.12817690028683, recent_avg -226\n",
      "Episode: 478, Steps: -117.09472044978648, recent_avg -222\n",
      "Episode: 479, Steps: -128.80487343963654, recent_avg -192\n",
      "Episode: 480, Steps: -453.9263989871246, recent_avg -198\n",
      "Episode: 481, Steps: -610.7036148131892, recent_avg -243\n",
      "Episode: 482, Steps: -120.38942169946776, recent_avg -234\n",
      "Episode: 483, Steps: -558.9544713214352, recent_avg -254\n",
      "Episode: 484, Steps: -125.22728416042784, recent_avg -253\n",
      "Episode: 485, Steps: -412.5013522461742, recent_avg -281\n",
      "Episode: 486, Steps: -542.4285501641955, recent_avg -320\n",
      "Episode: 487, Steps: -50.028236344625924, recent_avg -312\n",
      "Episode: 488, Steps: -426.13486374527474, recent_avg -343\n",
      "Episode: 489, Steps: -134.26312415518436, recent_avg -343\n",
      "Episode: 490, Steps: -551.1363632110445, recent_avg -353\n",
      "Episode: 491, Steps: -386.7038088050091, recent_avg -331\n",
      "Episode: 492, Steps: -96.49298835814943, recent_avg -328\n",
      "Episode: 493, Steps: 56.41580088686058, recent_avg -267\n",
      "Episode: 494, Steps: -407.32222104763804, recent_avg -295\n",
      "Episode: 495, Steps: -373.4242799503498, recent_avg -291\n",
      "Episode: 496, Steps: -360.6135579760497, recent_avg -273\n",
      "Episode: 497, Steps: -155.81487860449477, recent_avg -284\n",
      "Episode: 498, Steps: -381.82756089206674, recent_avg -279\n",
      "Episode: 499, Steps: -193.96992574732144, recent_avg -285\n"
     ]
    }
   ],
   "source": [
    "my_agent = Agent('LunarLander-v2',enable_tb_logging=True)\n",
    "rewards_buffer = []\n",
    "max_episodes = 500\n",
    "mean_score_buffer = deque(maxlen=10)\n",
    "for i in range(0,max_episodes):\n",
    "    my_agent.reset_env_state()\n",
    "    done = False\n",
    "    rewards_episodic = 0\n",
    "    env_steps = 0\n",
    "    while not(done):\n",
    "        state, action, reward, next_state, done = my_agent.play_one_step()\n",
    "        rewards_episodic+= reward\n",
    "        env_steps += 1\n",
    "        reward = reward if env_steps<=999 else -100\n",
    "        #my_agent.buffer.add_exp(state, action, reward, next_state, done)\n",
    "        my_agent.training_step()\n",
    "        \n",
    "    my_agent.apply_epsilon_decay()\n",
    "    rewards_buffer.append(rewards_episodic)\n",
    "    my_agent.hard_target_update()\n",
    "    mean_score = round(np.mean(rewards_buffer))\n",
    "    mean_score_buffer.append(rewards_episodic)\n",
    "    recent_mean = round(np.mean(list(mean_score_buffer)))\n",
    "    \n",
    "    if recent_mean >= 195.0:\n",
    "        print('DQN solved problem terminating...')\n",
    "        break\n",
    "        \n",
    "    print(\"\\rEpisode: {}, Steps: {}, recent_avg {}\".format(i, rewards_episodic, recent_mean, end=\"\"))\n",
    "\n",
    "    \n",
    "     \n",
    "          \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = gym.make('LunarLander-v2')\n",
    "test_state = env_test.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env': <gym.envs.box2d.lunar_lander.LunarLander at 0x26691bf8f70>,\n",
       " 'action_space': Discrete(4),\n",
       " 'observation_space': Box(-inf, inf, (8,), float32),\n",
       " 'reward_range': (-inf, inf),\n",
       " 'metadata': {'render.modes': ['human', 'rgb_array'],\n",
       "  'video.frames_per_second': 50},\n",
       " '_max_episode_steps': 1000,\n",
       " '_elapsed_steps': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_test.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tensorflow]",
   "language": "python",
   "name": "conda-env-Tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
