{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "renewable-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "import tensorboard\n",
    "from tensorflow.keras import backend as K\n",
    "from Sum_Tree import SumTree\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self,env_name):\n",
    "        \n",
    "        self.env = gym.make(env_name)\n",
    "        #self.env.seed(0)\n",
    "        self.env_state = self.env.reset()\n",
    "        self.lr = 0.1\n",
    "        self.gamma = .99\n",
    "        self.epsilon_decay_rate = .8\n",
    "        self.initializer = tf.keras.initializers.HeUniform()\n",
    "        self.loss_parameter = 1.0\n",
    "        self.loss_fn = tf.keras.losses.Huber(delta=self.loss_parameter)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.n_states = self.env.observation_space.shape[0]\n",
    "        self.tau = 0.10\n",
    "        self.batch_size = 32\n",
    "        self.epsilon = .5\n",
    "        self.min_epsilon = .05\n",
    "        self.dueling_method = 'max'\n",
    "        self.episodic_rewards = 0\n",
    "        self.episode_counter = 0\n",
    "        self.episode_time_step_counter = 0\n",
    "        self.target_update_method = 'hard'\n",
    "        self.online = self.make_nn()\n",
    "        self.target = self.make_target()\n",
    "        self.capacity = 5000\n",
    "        self.buffer = ReplayBuffer(self.capacity)\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "    def reset_env(self):\n",
    "        self.episode_counter+=1\n",
    "        self.episodic_rewards = 0\n",
    "        self.episode_time_step_counter = 0\n",
    "        self.env_state = self.env.reset()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def make_nn(self):\n",
    "        \n",
    "\n",
    "        Dueling_Input = tf.keras.layers.Input(shape=(self.n_states,),name='Dueling_Input')\n",
    "        \n",
    "        #layer_1_Dense = tf.keras.layers.Dense(64,activation='swish',kernel_initializer=self.initializer,name='layer_1_Dense')(Dueling_Input)\n",
    "        \n",
    "        layer_2_Dense = tf.keras.layers.Dense(64,activation='swish',kernel_initializer=self.initializer,name='layer_2_Dense')(Dueling_Input)\n",
    "        \n",
    "        #layer_3_Dense = tf.keras.layers.Dense(24,activation='swish',kernel_initializer=self.initializer,name='layer_3_Dense')(layer_2_Dense)\n",
    "        \n",
    "        Advantage_Layer = tf.keras.layers.Dense(32,activation='swish',kernel_initializer=self.initializer,name='Advantage_Layer')(layer_2_Dense)\n",
    "        \n",
    "        Value_Layer = tf.keras.layers.Dense(32,activation='swish',kernel_initializer=self.initializer,name='Value_Layer')(layer_2_Dense)\n",
    "        \n",
    "        Advantage_Layer2 = tf.keras.layers.Dense(self.n_actions,activation='linear',kernel_initializer=self.initializer,name='Advantage_Layer2')(Advantage_Layer)\n",
    "        \n",
    "        Value_Layer2 = tf.keras.layers.Dense(1,activation='linear',kernel_initializer=self.initializer,name='Value_Layer2')(Value_Layer)\n",
    "\n",
    "        if self.dueling_method == 'average':\n",
    "            \n",
    "            Value_Expanded = tf.keras.layers.Lambda(lambda s: K.expand_dims(s[:,0],-1),output_shape=(self.n_actions,))(Value_Layer2)\n",
    "            \n",
    "            Average_Advantage = tf.keras.layers.Lambda(lambda a: a[:,:] - K.mean(a[:,:],keepdims=True),output_shape=(self.n_states,))(Advantage_Layer2)\n",
    "            \n",
    "            online_output = tf.keras.layers.Add()([Value_Expanded,Average_Advantage])\n",
    "\n",
    "\n",
    "        else:\n",
    "            \n",
    "            Value_Expanded = tf.keras.layers.Lambda(lambda s: K.expand_dims(s[:,0],-1),output_shape=(self.n_actions,))(Value_Layer2)\n",
    "            \n",
    "            Average_Advantage = tf.keras.layers.Lambda(lambda a: a[:,:] - K.max(a[:,:],keepdims=True),output_shape=(self.n_states,))(Advantage_Layer2)\n",
    "            \n",
    "            online_output = tf.keras.layers.Add()([Value_Expanded,Average_Advantage])\n",
    "\n",
    "        online = tf.keras.Model(Dueling_Input,online_output)\n",
    "        online.compile(loss=self.loss_fn,optimizer=self.optimizer,metrics=['accuracy'])\n",
    "        return online\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    def make_target(self):\n",
    "        target = tf.keras.models.clone_model(self.online)\n",
    "        target.set_weights(self.online.get_weights())\n",
    "        return target\n",
    "        \n",
    "        \n",
    "    def epsilon_greedy_policy(self):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return np.random.randint(low=0,high=self.n_actions)\n",
    "        else:\n",
    "            Q_values = self.online.predict(np.reshape(self.env_state,(1,self.n_states)))\n",
    "            return np.argmax(Q_values[0])\n",
    "        \n",
    "    def apply_epsilon_decay(self):\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay_rate,self.min_epsilon)   \n",
    "        \n",
    "\n",
    "    def play_one_step(self):\n",
    "        state = np.array(self.env_state)\n",
    "        action = self.epsilon_greedy_policy()\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        self.buffer.add_exp(state=state, action=action, reward=reward, next_state=next_state, done=done)\n",
    "        self.env_state = next_state\n",
    "        self.episodic_rewards += reward\n",
    "        self.episode_time_step_counter += 1\n",
    "        return state, action, reward, next_state, done\n",
    "        \n",
    "         \n",
    "    def training_step(self):\n",
    "        \n",
    "        if len(self.buffer)<self.batch_size:\n",
    "            batch_size_instance = len(self.buffer)\n",
    "        else:\n",
    "            batch_size_instance = self.batch_size\n",
    "        \n",
    "        states,actions,rewards,next_states,dones = self.buffer.exp_sample(batch_size=batch_size_instance)\n",
    "        online_state_prediction = self.online.predict(states)\n",
    "        online_state_prediction_array = np.array(online_state_prediction)\n",
    "        online_next_state_prediciton = self.online.predict(next_states)\n",
    "        target_next_state_prediction = self.target.predict(next_states)\n",
    "        online_next_state_arg_max = tf.argmax(online_next_state_prediciton,axis=1)\n",
    "        double_learning_mask = tf.one_hot(online_next_state_arg_max,depth=self.n_actions)\n",
    "        masked_target_qs = tf.reduce_sum(target_next_state_prediction*double_learning_mask,axis=1,keepdims=True)\n",
    "        q_update = rewards + (1 - dones) * self.gamma * masked_target_qs\n",
    "        q_update = tf.squeeze(q_update,axis=-1)\n",
    "        \n",
    "        for i in range(0,online_state_prediction.shape[0]):\n",
    "            online_state_prediction_array[i,actions[i]] = q_update[i]\n",
    "        self.online.train_on_batch(states,online_state_prediction_array)\n",
    "            \n",
    "     \n",
    "    def update_target_network(self):\n",
    "        \n",
    "        if self.target_update_method == 'soft':\n",
    "            target_weights_current = self.target.get_weights()\n",
    "            online_weights_current = self.online.get_weights()\n",
    "            layer_counter = 0\n",
    "            for tw, ow in zip(target_weights_current,online_weights_current):\n",
    "                new_target_weights = (1-self.tau)*tw + (self.tau)*ow\n",
    "                target_weights_current[layer_counter] = new_target_weights\n",
    "                layer_counter += 1\n",
    "            self.target.set_weights(target_weights_current)\n",
    "        \n",
    "        else:\n",
    "            self.target.set_weights(self.online.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fabulous-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self,max_len):\n",
    "        self.max_len = max_len\n",
    "        self.buffer = deque(maxlen=self.max_len)\n",
    "        \n",
    "    \n",
    "    def add_exp(self,state,action,reward,next_state,done):\n",
    "        exp = (state,action,reward, next_state, done)\n",
    "        if len(self.buffer)<= self.max_len:\n",
    "            self.buffer.append(exp)\n",
    "        else:\n",
    "            self.buffer[0] = exp\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def exp_sample(self,batch_size):\n",
    "        indices = np.random.randint(len(self.buffer), size=batch_size)\n",
    "        batch = [self.buffer[index] for index in indices]\n",
    "        states, actions, rewards, next_states, dones = [np.array([experience[entry] for experience in batch])for entry in range(5)]\n",
    "        return states, actions, rewards[:,np.newaxis], next_states, dones[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recorded-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent = Agent('CartPole-v0')\n",
    "rewards_buffer = []\n",
    "max_episodes = 150\n",
    "mean_score_buffer = deque(maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-planning",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode: 0, Episode_Reward: 41, 10_Episode_Reward_Avg 41\n",
      " Episode: 1, Episode_Reward: 23, 10_Episode_Reward_Avg 32\n",
      " Episode: 2, Episode_Reward: 14, 10_Episode_Reward_Avg 26\n",
      " Episode: 3, Episode_Reward: 24, 10_Episode_Reward_Avg 26\n",
      " Episode: 4, Episode_Reward: 15, 10_Episode_Reward_Avg 23\n",
      " Episode: 5, Episode_Reward: 14, 10_Episode_Reward_Avg 18\n",
      " Episode: 6, Episode_Reward: 10, 10_Episode_Reward_Avg 15\n",
      " Episode: 7, Episode_Reward: 11, 10_Episode_Reward_Avg 15\n",
      " Episode: 8, Episode_Reward: 8, 10_Episode_Reward_Avg 12\n",
      " Episode: 9, Episode_Reward: 10, 10_Episode_Reward_Avg 11\n",
      " Episode: 10, Episode_Reward: 19, 10_Episode_Reward_Avg 12\n",
      " Episode: 11, Episode_Reward: 10, 10_Episode_Reward_Avg 12\n",
      " Episode: 12, Episode_Reward: 12, 10_Episode_Reward_Avg 12\n",
      " Episode: 13, Episode_Reward: 16, 10_Episode_Reward_Avg 13\n",
      " Episode: 14, Episode_Reward: 11, 10_Episode_Reward_Avg 14\n",
      " Episode: 15, Episode_Reward: 9, 10_Episode_Reward_Avg 12\n",
      " Episode: 16, Episode_Reward: 23, 10_Episode_Reward_Avg 14\n",
      " Episode: 17, Episode_Reward: 41, 10_Episode_Reward_Avg 20\n",
      " Episode: 18, Episode_Reward: 29, 10_Episode_Reward_Avg 23\n",
      " Episode: 19, Episode_Reward: 28, 10_Episode_Reward_Avg 26\n",
      " Episode: 20, Episode_Reward: 10, 10_Episode_Reward_Avg 26\n",
      " Episode: 21, Episode_Reward: 10, 10_Episode_Reward_Avg 24\n",
      " Episode: 22, Episode_Reward: 9, 10_Episode_Reward_Avg 17\n",
      " Episode: 23, Episode_Reward: 9, 10_Episode_Reward_Avg 13\n",
      " Episode: 24, Episode_Reward: 21, 10_Episode_Reward_Avg 12\n",
      " Episode: 25, Episode_Reward: 9, 10_Episode_Reward_Avg 12\n",
      " Episode: 26, Episode_Reward: 48, 10_Episode_Reward_Avg 19\n",
      " Episode: 27, Episode_Reward: 9, 10_Episode_Reward_Avg 19\n",
      " Episode: 28, Episode_Reward: 9, 10_Episode_Reward_Avg 19\n",
      " Episode: 29, Episode_Reward: 13, 10_Episode_Reward_Avg 18\n",
      " Episode: 30, Episode_Reward: 9, 10_Episode_Reward_Avg 18\n",
      " Episode: 31, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 32, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 33, Episode_Reward: 10, 10_Episode_Reward_Avg 10\n",
      " Episode: 34, Episode_Reward: 30, 10_Episode_Reward_Avg 14\n",
      " Episode: 35, Episode_Reward: 19, 10_Episode_Reward_Avg 16\n",
      " Episode: 36, Episode_Reward: 20, 10_Episode_Reward_Avg 18\n",
      " Episode: 37, Episode_Reward: 18, 10_Episode_Reward_Avg 19\n",
      " Episode: 38, Episode_Reward: 13, 10_Episode_Reward_Avg 20\n",
      " Episode: 39, Episode_Reward: 20, 10_Episode_Reward_Avg 18\n",
      " Episode: 40, Episode_Reward: 11, 10_Episode_Reward_Avg 16\n",
      " Episode: 41, Episode_Reward: 10, 10_Episode_Reward_Avg 14\n",
      " Episode: 42, Episode_Reward: 10, 10_Episode_Reward_Avg 13\n",
      " Episode: 43, Episode_Reward: 13, 10_Episode_Reward_Avg 13\n",
      " Episode: 44, Episode_Reward: 37, 10_Episode_Reward_Avg 16\n",
      " Episode: 45, Episode_Reward: 14, 10_Episode_Reward_Avg 17\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,max_episodes):\n",
    "    my_agent.reset_env()\n",
    "    done = False\n",
    "    rewards_episodic = 0\n",
    "    while not(done):\n",
    "        state, action, reward, next_state, done = my_agent.play_one_step()\n",
    "        \n",
    "        if not done or i == my_agent.env._max_episode_steps-1:\n",
    "            reward = reward\n",
    "        else:\n",
    "            reward = -100\n",
    "        \n",
    "        #rewards_episodic += reward\n",
    "        rewards_episodic += 1\n",
    "        my_agent.training_step()\n",
    "        \n",
    "        \n",
    "    \n",
    "    my_agent.apply_epsilon_decay()\n",
    "    my_agent.update_target_network()\n",
    "    \n",
    "    rewards_buffer.append(round(rewards_episodic,2))\n",
    "    mean_score = round(np.mean(rewards_buffer))\n",
    "    mean_score_buffer.append(rewards_episodic)\n",
    "    recent_mean = round(np.mean(list(mean_score_buffer)))\n",
    "    \n",
    "    print(\"\\r Episode: {}, Episode_Reward: {}, 10_Episode_Reward_Avg {}\".format(i, round(rewards_episodic,2), recent_mean, end=\"\"))\n",
    "    \n",
    "    if recent_mean >= 195.0:\n",
    "        print('DQN solved problem terminating...')\n",
    "        break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-professor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tensorflow]",
   "language": "python",
   "name": "conda-env-Tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
